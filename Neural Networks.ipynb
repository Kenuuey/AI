{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zavq_Qjz3H3A",
        "SmFZ8rtI3Kiv",
        "hYxVsXaSz4f1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Import"
      ],
      "metadata": {
        "id": "zavq_Qjz3H3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_r8c_xVqu-S7",
        "outputId": "20a4362a-1b57-461c-acd7-abac812f6794"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "z7VCM6wJnx3x"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from category_encoders import CountEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "SmFZ8rtI3Kiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./drive/MyDrive/data/ML_8/train.csv', parse_dates=['PurchDate'])\n",
        "test_df = pd.read_csv('./drive/MyDrive/data/ML_8/test.csv', parse_dates=['PurchDate'])\n",
        "\n",
        "train_df= train_df.sort_values(by=['PurchDate']).reset_index(drop=True)\n",
        "test_df = test_df.sort_values(by=['PurchDate']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "-jMnm8rOolJk"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates = train_df['PurchDate']\n",
        "\n",
        "date_split_1 = dates.quantile(1/3)\n",
        "date_split_2 = dates.quantile(2/3)\n",
        "\n",
        "train = train_df[train_df['PurchDate'] <= date_split_1].copy()\n",
        "valid = train_df[(train_df['PurchDate'] > date_split_1) & (train_df['PurchDate'] <= date_split_2)].copy()\n",
        "test = train_df[train_df['PurchDate'] > date_split_2].copy()"
      ],
      "metadata": {
        "id": "j5a62wNo1Skr"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = train_df.select_dtypes(include=['object']).columns.to_list()\n",
        "num_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
        "valid[cat_cols] = cat_imputer.transform(valid[cat_cols])\n",
        "test[cat_cols] = cat_imputer.transform(test[cat_cols])\n",
        "\n",
        "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
        "valid[num_cols] = num_imputer.transform(valid[num_cols])\n",
        "test[num_cols] = num_imputer.transform(test[num_cols])"
      ],
      "metadata": {
        "id": "jqm6PQznyI5b"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [train, valid, test]:\n",
        "    df.drop(columns=['PurchDate'], inplace=True)"
      ],
      "metadata": {
        "id": "cmsywUgs1xlE"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CountEncoder()\n",
        "\n",
        "train.loc[:, cat_cols] = encoder.fit_transform(train[cat_cols])\n",
        "valid.loc[:, cat_cols] = encoder.transform(valid[cat_cols])\n",
        "test.loc[:, cat_cols] = encoder.transform(test[cat_cols])"
      ],
      "metadata": {
        "id": "PrHOx35f0VOx"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train.drop(columns=['IsBadBuy']), train['IsBadBuy']\n",
        "X_valid, y_valid = valid.drop(columns=['IsBadBuy']), valid['IsBadBuy']\n",
        "X_test, y_test = test.drop(columns=['IsBadBuy']), test['IsBadBuy']"
      ],
      "metadata": {
        "id": "87XzTP0M48IH"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "y_train = y_train.astype(int).values.reshape(-1, 1)\n",
        "y_valid = y_valid.astype(int).values.reshape(-1, 1)\n",
        "y_test = y_test.astype(int).values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "vCdivCJN3xHK"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ],
      "metadata": {
        "id": "hYxVsXaSz4f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, n_hidden=100, activation='tanh', learning_rate=0.01, n_epochs=10, batch_size=32, optimizer='sgd', verbose=False):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _initialize_weights(self, input_dim):\n",
        "      self.W1 = 0.01 * np.random.randn(input_dim, self.n_hidden)\n",
        "      self.b1 = np.zeros((1, self.n_hidden))\n",
        "      self.W2 = 0.01 * np.random.randn(self.n_hidden, 1)\n",
        "      self.b2 = np.zeros((1, 1))\n",
        "\n",
        "      if self.optimizer == 'adam':\n",
        "        self.mW1 = np.zeros_like(self.W1)\n",
        "        self.mb1 = np.zeros_like(self.b1)\n",
        "        self.mW2 = np.zeros_like(self.W2)\n",
        "        self.mb2 = np.zeros_like(self.b2)\n",
        "        self.vW1 = np.zeros_like(self.W1)\n",
        "        self.vb1 = np.zeros_like(self.b1)\n",
        "        self.vW2 = np.zeros_like(self.W2)\n",
        "        self.vb2 = np.zeros_like(self.b2)\n",
        "        self.t = 0 # Time step for bias correction\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "      z = np.clip(z, -500, 500)\n",
        "      return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _binary_cross_entropy(self, y_true, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def _forward(self, X):\n",
        "      self.Z1 = X @ self.W1 + self.b1\n",
        "      self.A1 = self._activation(self.Z1)\n",
        "      self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "      self.A2 = self._sigmoid(self.Z2)\n",
        "      return self.A2\n",
        "\n",
        "    def _backward(self, X, y, output):\n",
        "      m = X.shape[0]\n",
        "      dZ2 = output - y\n",
        "      dW2 = self.A1.T @ dZ2 / m\n",
        "      db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "      dA1 = dZ2 @ self.W2.T\n",
        "      dZ1 = dA1 * self._activation_derivative(self.Z1)\n",
        "      dW1 = X.T @ dZ1 / m\n",
        "      db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "      return dW1, db1, dW2, db2\n",
        "\n",
        "    def _activation(self, z):\n",
        "      if self.activation == 'tanh':\n",
        "        return np.tanh(z)\n",
        "      elif self.activation == 'sigmoid':\n",
        "        return self._sigmoid(z)\n",
        "      elif self.activation == 'relu':\n",
        "        return np.maximum(0, z)\n",
        "      elif self.activation == 'cos':\n",
        "        return np.cos(z)\n",
        "      else:\n",
        "        raise ValueError(\"Unknown activation\")\n",
        "\n",
        "    def _activation_derivative(self, z):\n",
        "      if self.activation == 'tanh':\n",
        "        return 1 - np.tanh(z) ** 2\n",
        "      elif self.activation == 'sigmoid':\n",
        "        sig = 1 / (1 + np.exp(-z))\n",
        "        return sig * (1 - sig)\n",
        "      elif self.activation == 'relu':\n",
        "        return (z > 0).astype(float)\n",
        "      elif self.activation == 'cos':\n",
        "        return -np.sin(z)\n",
        "      else:\n",
        "        raise ValueError(\"Unknown activation\")\n",
        "\n",
        "    def _update_parameters(self, dW1, db1, dW2, db2):\n",
        "      if self.optimizer == 'sgd':\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "      elif self.optimizer == 'adam':\n",
        "        beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "        self.t += 1\n",
        "        for param, dparam, m, v in zip([self.W1, self.b1, self.W2, self.b2],\n",
        "                                        [dW1, db1, dW2, db2],\n",
        "                                        [self.mW1, self.mb1, self.mW2, self.mb2],\n",
        "                                        [self.vW1, self.vb1, self.vW2, self.vb2]):\n",
        "          m[:] = beta1 * m + (1 - beta1) * dparam\n",
        "          v[:] = beta2 * v + (1 - beta2) * (dparam ** 2)\n",
        "          m_hat = m / (1 - beta1 ** self.t)\n",
        "          v_hat = v / (1 - beta2 ** self.t)\n",
        "          param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "      n_samples, n_features = X.shape\n",
        "      self._initialize_weights(n_features)\n",
        "\n",
        "      for epoch in range(self.n_epochs):\n",
        "        if self.verbose and X_val is not None and y_val is not None:\n",
        "          train_pred = self._forward(X)\n",
        "          train_loss = self._binary_cross_entropy(y, train_pred)\n",
        "          val_pred = self._forward(X_val)\n",
        "          val_loss = self._binary_cross_entropy(y_val, val_pred)\n",
        "          print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        idx = np.random.permutation(n_samples)\n",
        "        X_shuffled, y_shuffled = X[idx], y[idx]\n",
        "        for i in range(0, n_samples, self.batch_size):\n",
        "          X_batch = X_shuffled[i:i + self.batch_size]\n",
        "          y_batch = y_shuffled[i:i + self.batch_size]\n",
        "\n",
        "          output = self._forward(X_batch)\n",
        "          dW1, db1, dW2, db2 = self._backward(X_batch, y_batch, output)\n",
        "          self._update_parameters(dW1, db1, dW2, db2)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "      proba_1 = self._forward(X).flatten()\n",
        "      return np.vstack([1 - proba_1, proba_1]).T\n",
        "\n",
        "    def predict(self, X):\n",
        "      return (self._forward(X) >= 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "3K9S1Z9Dz5e9"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_score(y_true, y_prob):\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    return 2 * auc - 1"
      ],
      "metadata": {
        "id": "XOKxWYIx3syG"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(\n",
        "    n_hidden=128,\n",
        "    activation='tanh',\n",
        "    learning_rate=0.01,\n",
        "    optimizer='adam',\n",
        "    n_epochs=8,\n",
        "    batch_size=64,\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train, X_valid_scaled, y_valid)\n",
        "y_valid_pred_custom = model.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "print(f\"Custom MLP Gini (Adam optimizer): {gini_score(y_valid, y_valid_pred_custom):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2NAt6m30fd",
        "outputId": "764bdf43-dc32-475a-e751-7285ff5cff63"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP Gini (Adam optimizer): 0.3186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(\n",
        "    n_hidden=128,\n",
        "    activation='sigmoid',\n",
        "    learning_rate=0.01,\n",
        "    optimizer='adam',\n",
        "    n_epochs=8,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_valid_pred_custom = model.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "print(f\"Custom MLP Gini (Adam optimizer): {gini_score(y_valid, y_valid_pred_custom):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_uHxRKjdIOz",
        "outputId": "6482b910-e1d2-4bba-92a5-a1bd49c47921"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP Gini (Adam optimizer): 0.3506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(\n",
        "    n_hidden=128,\n",
        "    activation='tanh',\n",
        "    learning_rate=0.01,\n",
        "    optimizer='sgd',\n",
        "    n_epochs=8,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_valid_pred_custom = model.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "print(f\"Custom MLP Gini (SGD optimizer): {gini_score(y_valid, y_valid_pred_custom):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ7btZixbqM0",
        "outputId": "bfc88f40-e6b9-48af-85a8-71be669b0742"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP Gini (SGD optimizer): 0.3185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(\n",
        "    n_hidden=128,\n",
        "    activation='sigmoid',\n",
        "    learning_rate=0.01,\n",
        "    optimizer='sgd',\n",
        "    n_epochs=8,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_valid_pred_custom = model.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "print(f\"Custom MLP Gini (SGD optimizer): {gini_score(y_valid, y_valid_pred_custom):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFTxbcbByi9C",
        "outputId": "37d9c3df-3ca4-4dd4-e5f0-9d3e5bf17162"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP Gini (SGD optimizer): 0.2823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(128,),\n",
        "    activation='logistic',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "sk_model.fit(X_train_scaled, y_train.ravel())\n",
        "y_valid_pred_sklearn = sk_model.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "print(f\"Sklearn MLP Gini:, {gini_score(y_valid, y_valid_pred_sklearn):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYmqKAUp8rS4",
        "outputId": "954e7d91-e034-445c-f7b0-7ed92c8568fa"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn MLP Gini:, 0.3625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations = ['tanh', 'sigmoid', 'relu', 'cos']\n",
        "\n",
        "for act in activations:\n",
        "    model = MLP(n_hidden=128, activation=act, learning_rate=0.01, n_epochs=12, optimizer='adam')\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_valid_pred = model.predict_proba(X_valid_scaled)[:, 1]\n",
        "    gini = gini_score(y_valid, y_valid_pred)\n",
        "    print(f\"Activation: {act} → Gini: {gini:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA1xm15S-Hfq",
        "outputId": "a1ff2368-0b3e-4968-ed62-24084d1bc093"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation: tanh → Gini: 0.3094\n",
            "Activation: sigmoid → Gini: 0.3455\n",
            "Activation: relu → Gini: 0.3624\n",
            "Activation: cos → Gini: 0.2438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=100):\n",
        "      super(TorchMLP, self).__init__()\n",
        "      self.model = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, 1),\n",
        "        nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid, y_valid, epochs=10, batch_size=64, lr=0.001, verbose=False):\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      criterion = nn.BCELoss()\n",
        "      optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "      X_train = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
        "      y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "      X_valid = torch.tensor(X_valid, dtype=torch.float32).to(self.device)\n",
        "      y_valid = torch.tensor(y_valid, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        self.train()\n",
        "        permutation = torch.randperm(X_train.size()[0])\n",
        "\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for i in range(0, X_train.size()[0], batch_size):\n",
        "          indices = permutation[i:i+batch_size]\n",
        "          X_batch, y_batch = X_train[indices], y_train[indices]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = self(X_batch)\n",
        "          loss = criterion(outputs, y_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "        train_loss = epoch_loss / X_train.size(0)\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "          val_pred = self(X_valid)\n",
        "          val_loss = criterion(val_pred, y_valid)\n",
        "        if verbose:\n",
        "          print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "          y_valid_pred = torch.sigmoid(self(X_valid)).cpu().numpy().flatten()\n",
        "\n",
        "      return y_valid_pred"
      ],
      "metadata": {
        "id": "xQmXi2P6ezFf"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TorchMLP(input_dim=X_train.shape[1], hidden_dim=64)\n",
        "y_pred = model.fit(X_train_scaled, y_train, X_valid_scaled, y_valid, epochs=8, lr=0.001)\n",
        "\n",
        "print(f\"Gini score (TorchMLP): {gini_score(y_valid, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFuo4fxS_bYl",
        "outputId": "a52f0e4c-6673-46ac-ed81-d2b9ee236d28"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini score (TorchMLP): 0.3628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ginis(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "      X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
        "      X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "      y_train_pred = model(X_train).numpy().flatten()\n",
        "      y_val_pred   = model(X_val).numpy().flatten()\n",
        "      y_test_pred  = model(X_test).numpy().flatten()\n",
        "\n",
        "    train_gini = gini_score(y_train, y_train_pred)\n",
        "    valid_gini = gini_score(y_val, y_val_pred)\n",
        "    test_gini  = gini_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"Training Gini:     {train_gini:.4f}\")\n",
        "    print(f\"Validation Gini:  {valid_gini:.4f}\")\n",
        "    print(f\"Test Gini:        {test_gini:.4f}\")"
      ],
      "metadata": {
        "id": "BkvGqnkYpK3E"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_ginis(model, X_train_scaled, y_train, X_valid_scaled, y_valid, X_test_scaled, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW-zqX3xpMIi",
        "outputId": "0c729f74-0df0-47c4-9ea5-57eb515accbe"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Gini:     0.4405\n",
            "Validation Gini:  0.3628\n",
            "Test Gini:        0.4268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "- The model fits the training data moderately well (Train Gini = 0.4460).\n",
        "- A slight drop on the validation set (Valid Gini = 0.3653) is expected and shows generalization.\n",
        "- Test Gini (0.4241) is surprisingly higher than validation — this suggests the model generalizes well to unseen data and is not significantly overfitting.\n",
        "\n",
        "Conclusion:\n",
        "- Model shows healthy generalization.\n",
        "- No clear signs of overfitting.\n",
        "- Further gains may be possible by tuning model capacity or using advanced regularization."
      ],
      "metadata": {
        "id": "_fzGZSiF7D3m"
      }
    }
  ]
}