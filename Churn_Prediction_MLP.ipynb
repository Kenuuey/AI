{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YD8VVw_kNyvO",
        "gwRT-ZLHNw5Q",
        "Y3v-XaaCC3M2",
        "dS6iuSQZE-GF",
        "fItg_qsiC5TU",
        "sJv2gyGlYC8w",
        "p_WSCzjNYHgm",
        "81CmosoxZrC5",
        "eIln933QbO4H",
        "BT-YWYCmbTY_"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "YD8VVw_kNyvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0IWqaXP49qo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "gwRT-ZLHNw5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path = './drive/MyDrive/Projects/data/DS_2/'\n",
        "train = pd.read_csv(path + 'bank_data_train.csv')\n",
        "test = pd.read_csv(path + 'bank_data_test.csv')\n",
        "\n",
        "# Target column\n",
        "target = 'TARGET'\n",
        "\n",
        "# Basic info\n",
        "print(train.shape, test.shape)\n",
        "print(train[target].value_counts(normalize=True))\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(columns=target)\n",
        "y = train[target]\n",
        "\n",
        "# Split train into train + validation stratified\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "num_cols = X_train.select_dtypes(exclude='object').columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(\"Numerical columns:\", num_cols)\n",
        "print(\"Categorical columns:\", cat_cols)\n",
        "\n",
        "# Impute missing values\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
        "X_val[num_cols] = num_imputer.transform(X_val[num_cols])\n",
        "test[num_cols] = num_imputer.transform(test[num_cols])\n",
        "\n",
        "X_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\n",
        "X_val[cat_cols] = cat_imputer.transform(X_val[cat_cols])\n",
        "test[cat_cols] = cat_imputer.transform(test[cat_cols])\n",
        "\n",
        "# Encode categorical features with frequency encoding\n",
        "for col in cat_cols:\n",
        "    freq = X_train[col].value_counts(normalize=True)\n",
        "    X_train[col] = X_train[col].map(freq)\n",
        "    X_val[col] = X_val[col].map(freq).fillna(0)\n",
        "    test[col] = test[col].map(freq).fillna(0)\n",
        "\n",
        "# Clip outliers using IQR method\n",
        "for col in num_cols:\n",
        "    Q1 = X_train[col].quantile(0.25)\n",
        "    Q3 = X_train[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    X_train[col] = X_train[col].clip(lower, upper)\n",
        "    X_val[col] = X_val[col].clip(lower, upper)\n",
        "    test[col] = test[col].clip(lower, upper)\n",
        "\n",
        "# Scale all features\n",
        "scaler = StandardScaler()\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
        "test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns)\n",
        "\n",
        "print(\"Preprocessing complete.\")"
      ],
      "metadata": {
        "id": "EIgHtBC6bAz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './drive/MyDrive/Projects/data/DS_2/'"
      ],
      "metadata": {
        "id": "7ndtmnhaBY39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(path + 'bank_data_train.csv')\n",
        "test = pd.read_csv(path + 'bank_data_test.csv')"
      ],
      "metadata": {
        "id": "BAm62a2bZUg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling missing values"
      ],
      "metadata": {
        "id": "XCr-HBzOLF3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = train.select_dtypes(exclude='object').columns\n",
        "cat_cols = train.select_dtypes(include='object').columns\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
        "test[num_cols] = num_imputer.transform(test[num_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
        "test[cat_cols] = cat_imputer.transform(test[cat_cols])"
      ],
      "metadata": {
        "id": "M4M1kr3TL2p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting features and target"
      ],
      "metadata": {
        "id": "IvYXqHv4STva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(columns='TARGET')\n",
        "y = train['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nUVUejeSTRC",
        "outputId": "83e3f5ee-ef12-45fe-a97d-013e29dec091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284152, 115)\n",
            "(284152,)\n",
            "(71038, 115)\n",
            "(71038,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical Variables"
      ],
      "metadata": {
        "id": "dbD6_iZ3LKru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cat_cols:\n",
        "  freq = X_train[col].value_counts(normalize=True)\n",
        "  X_train[col] = X_train[col].map(freq)\n",
        "  X_test[col] = X_test[col].map(freq).fillna(0)"
      ],
      "metadata": {
        "id": "sJsH9m-GSPTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Outliers"
      ],
      "metadata": {
        "id": "tKIxMoP3LM2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "  Q1 = X_train[col].quantile(0.25)\n",
        "  Q3 = X_train[col].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  lower = Q1 - 1.5 * IQR\n",
        "  upper = Q3 + 1.5 * IQR\n",
        "  X_train[col] = X_train[col].clip(lower, upper)\n",
        "  X_test[col] = X_test[col].clip(lower, upper)"
      ],
      "metadata": {
        "id": "3OIbnLXlU7L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale Numerical Features"
      ],
      "metadata": {
        "id": "hIHmZDq2LRPs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A9wlyNvYVtK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Train class distribution:\", Counter(y_train))\n",
        "print(\"Test class distribution:\", Counter(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6hKG1FaYTl",
        "outputId": "e29148c0-c3f3-4e68-d154-5b8b2ed972ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: Counter({0.0: 261012, 1.0: 23140})\n",
            "Test class distribution: Counter({0.0: 65253, 1.0: 5785})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Classifier"
      ],
      "metadata": {
        "id": "Y3v-XaaCC3M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier(strategy='most_frequent')\n",
        "dummy.fit(X_train, y_train)\n",
        "y_pred_dummy = dummy.predict(X_val)\n",
        "y_proba_dummy = dummy.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Dummy Classifier metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_dummy))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_dummy))\n",
        "print(classification_report(y_val, y_pred_dummy))\n"
      ],
      "metadata": {
        "id": "EEWG82bVbDgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
      ],
      "metadata": {
        "id": "mjpDcgqRWdPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy.fit(X_train, y_train)\n",
        "y_pred_dummy = dummy.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dummy))\n",
        "print(classification_report(y_test, y_pred_dummy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7FmnwVC49q",
        "outputId": "22d230a6-722f-49fe-a2f1-6425eb4af2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.918564711844365\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96     65253\n",
            "         1.0       0.00      0.00      0.00      5785\n",
            "\n",
            "    accuracy                           0.92     71038\n",
            "   macro avg       0.46      0.50      0.48     71038\n",
            "weighted avg       0.84      0.92      0.88     71038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "dS6iuSQZE-GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=2)\n",
        "\n",
        "start_time = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Random Forest GridSearch done in {end_time - start_time:.2f}s\")\n",
        "print(\"Best params:\", grid_search.best_params_)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_val)\n",
        "y_proba_rf = best_rf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Random Forest metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_rf))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_rf))\n",
        "print(classification_report(y_val, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "XF_LsgzgbFOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "start_time = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(f\"Grid search training time: {training_time:.2f} seconds\")\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "xmM1EFryXO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scikit-learn MLPClassifier"
      ],
      "metadata": {
        "id": "fItg_qsiC5TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(64, 32),\n",
        "                    activation='relu',\n",
        "                    solver='adam',\n",
        "                    max_iter=200,\n",
        "                    random_state=42,\n",
        "                    early_stopping=True,\n",
        "                    class_weight='balanced')\n",
        "\n",
        "start_time = time.time()\n",
        "mlp.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"MLPClassifier training time: {end_time - start_time:.2f}s\")\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_val)\n",
        "y_proba_mlp = mlp.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"MLPClassifier metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_mlp))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_mlp))\n",
        "print(classification_report(y_val, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "nPrWGyFwbHR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu',\n",
        "                    solver='adam', max_iter=200, random_state=42,\n",
        "                    class_weight='balanced')\n",
        "\n",
        "start_time = time.time()\n",
        "mlp.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(f\"MLP training time: {training_time:.2f} seconds\")\n",
        "\n",
        "y_pred_mlp = mlp.predict(X_test)\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_mlp))\n",
        "\n",
        "y_proba_mlp = mlp.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_proba_mlp)\n",
        "print(\"ROC AUC on test set:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8biQoeDXemf",
        "outputId": "45135b45-95a1-45f4-f22b-ee8031a3b035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP training time: 269.10 seconds\n",
            "Accuracy on test set: 0.918564711844365\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96     65253\n",
            "         1.0       0.00      0.00      0.00      5785\n",
            "\n",
            "    accuracy                           0.92     71038\n",
            "   macro avg       0.46      0.50      0.48     71038\n",
            "weighted avg       0.84      0.92      0.88     71038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras (TensorFlow High-Level API)"
      ],
      "metadata": {
        "id": "sJv2gyGlYC8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "y_proba_keras = model.predict(X_val).flatten()\n",
        "y_pred_keras = (y_proba_keras > 0.5).astype(int)\n",
        "\n",
        "print(\"Keras MLP metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_keras))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_keras))\n",
        "print(classification_report(y_val, y_pred_keras))\n"
      ],
      "metadata": {
        "id": "7yeyZM4qbI7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=2)\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(f\"Keras model training time: {training_time:.2f} seconds\")\n",
        "\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred_keras = (y_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_keras))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_keras))"
      ],
      "metadata": {
        "id": "kSA4Ip1kCSXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow (Low-Level API)"
      ],
      "metadata": {
        "id": "p_WSCzjNYHgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
        "        self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        return self.out(x)\n",
        "\n",
        "# Prepare datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).batch(64).shuffle(10000)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values)).batch(64)\n",
        "\n",
        "model = SimpleMLP()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "patience = 5\n",
        "best_val_loss = np.inf\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    # Training\n",
        "    for x_batch, y_batch in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Validation\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    for x_batch, y_batch in val_ds:\n",
        "        val_logits = model(x_batch, training=False)\n",
        "        val_loss = loss_fn(y_batch, val_logits)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        preds = tf.cast(val_logits > 0.5, tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(preds[:, 0] == tf.cast(y_batch, tf.int32), tf.float32))\n",
        "        val_accuracies.append(acc.numpy())\n",
        "\n",
        "    val_loss_avg = np.mean(val_losses)\n",
        "    val_acc_avg = np.mean(val_accuracies)\n",
        "\n",
        "    print(f\"Validation loss: {val_loss_avg:.4f}, Accuracy: {val_acc_avg:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss_avg < best_val_loss:\n",
        "        best_val_loss = val_loss_avg\n",
        "        wait = 0\n",
        "        model.save_weights(\"best_weights.tf\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load best weights\n",
        "model.load_weights(\"best_weights.tf\")\n",
        "\n",
        "# Evaluate on val set\n",
        "y_proba_tf = model.predict(X_val).flatten()\n",
        "y_pred_tf = (y_proba_tf > 0.5).astype(int)\n",
        "\n",
        "print(\"TensorFlow MLP metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_tf))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_tf))\n",
        "print(classification_report(y_val, y_pred_tf))\n"
      ],
      "metadata": {
        "id": "tfIWTcNSbKf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.hidden2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        return self.out(x)\n",
        "\n",
        "model = SimpleMLP()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tf = tf.reshape(tf.convert_to_tensor(y_train, dtype=tf.float32), (-1, 1))\n",
        "\n",
        "start_time = time.time()\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(X_train_tf)\n",
        "        loss = loss_fn(y_train_tf, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"TensorFlow custom training time: {training_time:.2f} seconds\")\n",
        "\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_pred_probs = model(X_test_tf).numpy()\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).reshape(-1)\n",
        "\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "4KxtbOVwYH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy MLP"
      ],
      "metadata": {
        "id": "81CmosoxZrC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumpyMLP:\n",
        "    def __init__(self, input_dim, hidden1=64, hidden2=32, lr=0.001):\n",
        "        np.random.seed(42)\n",
        "        self.lr = lr\n",
        "        self.W1 = np.random.randn(input_dim, hidden1) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden1))\n",
        "        self.W2 = np.random.randn(hidden1, hidden2) * 0.01\n",
        "        self.b2 = np.zeros((1, hidden2))\n",
        "        self.W3 = np.random.randn(hidden2, 1) * 0.01\n",
        "        self.b3 = np.zeros((1, 1))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_deriv(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = self.relu(self.Z2)\n",
        "        self.Z3 = self.A2 @ self.W3 + self.b3\n",
        "        self.A3 = self.sigmoid(self.Z3)\n",
        "        return self.A3\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        m = y.shape[0]\n",
        "        dZ3 = output - y.reshape(-1, 1)\n",
        "        dW3 = (self.A2.T @ dZ3) / m\n",
        "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA2 = dZ3 @ self.W3.T\n",
        "        dZ2 = dA2 * self.relu_deriv(self.Z2)\n",
        "        dW2 = (self.A1.T @ dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * self.relu_deriv(self.Z1)\n",
        "        dW1 = (X.T @ dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights\n",
        "        self.W3 -= self.lr * dW3\n",
        "        self.b3 -= self.lr * db3\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=64):\n",
        "        n = X.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            perm = np.random.permutation(n)\n",
        "            X_shuffled = X[perm]\n",
        "            y_shuffled = y[perm]\n",
        "            for i in range(0, n, batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "                output = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, output)\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                pred = self.forward(X)\n",
        "                loss = -np.mean(y * np.log(pred + 1e-8) + (1 - y) * np.log(1 - pred + 1e-8))\n",
        "                print(f\"Epoch {epoch + 1}/{epochs} - loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        prob = self.forward(X)\n",
        "        return (prob > 0.5).astype(int), prob\n",
        "\n",
        "# Train numpy MLP\n",
        "np_mlp = NumpyMLP(input_dim=X_train.shape[1], hidden1=64, hidden2=32, lr=0.001)\n",
        "\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values\n",
        "\n",
        "np_mlp.train(X_train_np, y_train_np, epochs=50, batch_size=128)\n",
        "\n",
        "X_val_np = X_val.values\n",
        "y_pred_np, y_proba_np = np_mlp.predict(X_val_np)\n",
        "\n",
        "print(\"NumPy MLP metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_np))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_val, y_proba_np))\n",
        "print(classification_report(y_val, y_pred_np))\n"
      ],
      "metadata": {
        "id": "-iBgJ46fbM5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Binary cross-entropy loss and derivative\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    # Clip for numerical stability\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_derivative(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    return (y_pred - y_true) / (y_pred * (1 - y_pred) * y_true.shape[0])\n",
        "\n",
        "# Initialize parameters\n",
        "input_dim = X_train.shape[1]\n",
        "hidden1_size = 64\n",
        "hidden2_size = 32\n",
        "output_size = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "# Xavier initialization\n",
        "W1 = np.random.randn(input_dim, hidden1_size) * np.sqrt(2 / input_dim)\n",
        "b1 = np.zeros((1, hidden1_size))\n",
        "\n",
        "W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2 / hidden1_size)\n",
        "b2 = np.zeros((1, hidden2_size))\n",
        "\n",
        "W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(2 / hidden2_size)\n",
        "b3 = np.zeros((1, output_size))\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "# Prepare data as numpy arrays\n",
        "X = X_train.values.astype(np.float32)\n",
        "y = y_train.values.reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "n_samples = X.shape[0]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle data each epoch\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = start + batch_size\n",
        "        X_batch = X[start:end]\n",
        "        y_batch = y[start:end]\n",
        "\n",
        "        # Forward pass\n",
        "        z1 = np.dot(X_batch, W1) + b1\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = np.dot(a1, W2) + b2\n",
        "        a2 = relu(z2)\n",
        "\n",
        "        z3 = np.dot(a2, W3) + b3\n",
        "        a3 = sigmoid(z3)  # output predictions\n",
        "\n",
        "        loss = binary_cross_entropy(y_batch, a3)\n",
        "        epoch_loss += loss * X_batch.shape[0]\n",
        "\n",
        "        # Backpropagation\n",
        "        dz3 = binary_cross_entropy_derivative(y_batch, a3) * sigmoid_derivative(z3)\n",
        "        dW3 = np.dot(a2.T, dz3)\n",
        "        db3 = np.sum(dz3, axis=0, keepdims=True)\n",
        "\n",
        "        da2 = np.dot(dz3, W3.T)\n",
        "        dz2 = da2 * relu_derivative(z2)\n",
        "        dW2 = np.dot(a1.T, dz2)\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = np.dot(dz2, W2.T)\n",
        "        dz1 = da1 * relu_derivative(z1)\n",
        "        dW1 = np.dot(X_batch.T, dz1)\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        W3 -= learning_rate * dW3\n",
        "        b3 -= learning_rate * db3\n",
        "\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "\n",
        "    epoch_loss /= n_samples\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Inference function\n",
        "def predict(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = relu(z2)\n",
        "\n",
        "    z3 = np.dot(a2, W3) + b3\n",
        "    a3 = sigmoid(z3)\n",
        "    return (a3 > 0.5).astype(int)\n",
        "\n",
        "# Evaluate on test set\n",
        "X_test_np = X_test.values.astype(np.float32)\n",
        "y_test_np = y_test.values.astype(np.int32)\n",
        "\n",
        "y_pred_np = predict(X_test_np)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(\"Test accuracy:\", accuracy_score(y_test_np, y_pred_np))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_np, y_pred_np))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_pred_np))\n"
      ],
      "metadata": {
        "id": "BbUdUXO-ZsjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Final Predictions on Test Set (Using best model)"
      ],
      "metadata": {
        "id": "eIln933QbO4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using Keras model for final prediction (you can choose your best model)\n",
        "\n",
        "test_proba = model.predict(test_scaled).flatten()\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test.index,\n",
        "    'TARGET': test_proba\n",
        "})\n",
        "\n",
        "submission.to_csv('final_predictions.csv', index=False)\n",
        "print(\"Saved final predictions to final_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "EMA9aSYzbPTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary table format"
      ],
      "metadata": {
        "id": "BT-YWYCmbTY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame([\n",
        "    ['DummyClassifier', 'Most Frequent', '-', accuracy_score(y_val, y_pred_dummy), roc_auc_score(y_val, y_proba_dummy)],\n",
        "    ['RandomForest', str(grid_search.best_params_), '-', accuracy_score(y_val, y_pred_rf), roc_auc_score(y_val, y_proba_rf)],\n",
        "    ['MLPClassifier', 'hidden_layer_sizes=(64,32)', '-', accuracy_score(y_val, y_pred_mlp), roc_auc_score(y_val, y_proba_mlp)],\n",
        "    ['Keras', '64,32 + dropout', 'Adam lr=0.001', accuracy_score(y_val, y_pred_keras), roc_auc_score(y_val, y_proba_keras)],\n",
        "    ['TensorFlow', '64,32 + dropout', 'Adam lr=0.001', accuracy_score(y_val, y_pred_tf), roc_auc_score(y_val, y_proba_tf)],\n",
        "    ['NumPy', '64,32 (manual)', 'lr=0.001', accuracy_score(y_val, y_pred_np), roc_auc_score(y_val, y_proba_np)],\n",
        "], columns=['Library', 'Hyperparameters', 'Notes', 'Accuracy', 'ROC_AUC'])\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "ryFU4D9nbTIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}