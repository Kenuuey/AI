{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ1whYjHXppV"
   },
   "source": [
    "ðŸ“¦ Library Descriptions:\r\n",
    "\r\n",
    "- **langchain-gigachat** â€” integration of GigaChat with LangChain, used for building request chains to LLMs.\r\n",
    "\r\n",
    "- **langgraph** â€” library for creating graph-based structures of agents and LangChain chains.\r\n",
    "\r\n",
    "- **langchain-community** â€” community-driven extensions and connectors for LangChain.\r\n",
    "\r\n",
    "- **langchain** â€” the core framework for working with LLMs, agents, memory, and pipelines.\r\n",
    "\r\n",
    "- **faiss-cpu** â€” library from Facebook (Facebook AI Similarity Search) for fast vector search on CPU, suitable for large-scale embeddings.\r\n",
    "\r\n",
    "- **sentence-transformers** â€” library for generating text embeddings using pretrained models from Hugging Face.\r\n",
    "\r\n",
    "- **playwright** â€” headless browser automation tool, often used for web scraping and parsing.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Norff_tVXNT3"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-gigachat\n",
    "!pip install langgraph\n",
    "!pip install langchain-community\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "!pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-1kBmdTmUdI",
    "outputId": "48bc128e-a0ef-4d2b-cff9-a3b92b595e7e"
   },
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0o6IlpAfmsdE",
    "outputId": "207bbf26-d724-4237-8426-083c5482404e"
   },
   "outputs": [],
   "source": [
    "%%writefile parser.py\n",
    "\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page with search results for the \"Physics\" tag\n",
    "url = \"https://nplus1.ru/search?tags=869\"\n",
    "\n",
    "# Initialize the asynchronous loader and fetch the page\n",
    "loader = AsyncChromiumLoader([url])\n",
    "html = loader.load()\n",
    "\n",
    "# Save the fetched page to a file (useful for debugging parsing)\n",
    "with open('page.html', 'w') as f:\n",
    "    f.write(html[0].page_content)\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html[0].page_content, 'html.parser')\n",
    "\n",
    "# CSS class of article links on N+1 website (manually identified after inspecting page structure)\n",
    "articles = 'n1_climb_4 transition-colors duration-75 hover:text-main inline-block mb-10 sm:mb-5 font-spectral leading-24'\n",
    "\n",
    "# Collect and save links\n",
    "links = []\n",
    "with open('links.txt', 'w') as f:\n",
    "    for link in soup.find_all('a', class_=articles):\n",
    "        print(link)  # Debug output\n",
    "        links.append(link['href'])\n",
    "        f.write(link['href'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FLzrV7EeBxE",
    "outputId": "203eec6d-dc76-4b41-b847-5188f0bcc1bf"
   },
   "outputs": [],
   "source": [
    "!python parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cm-pIyyCGG8B",
    "outputId": "302cb89b-f7f4-4bf3-8ad6-dac8273912e5"
   },
   "outputs": [],
   "source": [
    "with open('links.txt') as f:\n",
    "    sources = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "54kbntOtGREj"
   },
   "outputs": [],
   "source": [
    "# Import Hugging Face embeddings (weâ€™ll use pretrained models)\n",
    "!pip install langchain-huggingface\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Import different search strategies (BM25 â€” classic keyword-based, FAISS â€” vector-based)\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# FAISS â€” a vector database for storing embeddings of text chunks\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "# Document class is used to wrap texts into a format that LangChain understands\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# WebBaseLoader â€” a simple webpage loader (weâ€™ll use it to fetch article content from URLs)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# A text splitter that breaks documents into smaller chunks, respecting structure\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTfyaUIpHYpO",
    "outputId": "cb07f971-cee1-4ad1-d7a3-636ad3544967"
   },
   "outputs": [],
   "source": [
    "# Load articles by links using WebBaseLoader\n",
    "loader = WebBaseLoader(sources.split())  # Pass the list of links from the file\n",
    "docs = loader.load()  # Load the contents of all URLs as a list of documents\n",
    "\n",
    "# Check how many articles were successfully loaded\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2axrQUcH2rB",
    "outputId": "8c6f6645-8001-4916-c447-95f68d10995c"
   },
   "outputs": [],
   "source": [
    "# Set up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum chunk size\n",
    "    chunk_overlap=100      # Overlap between chunks\n",
    ")\n",
    "\n",
    "# Split documents into fragments\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Check how many chunks were created in total\n",
    "print(len(split_docs))\n",
    "\n",
    "# View one of the fragments (for example, the second one)\n",
    "split_docs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZULH_ckIH7d",
    "outputId": "d6533ef3-941a-44e1-b43b-9729ae5dce67"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Measure execution time of the vectorization step\n",
    "\n",
    "# Choose the model for generating embeddings\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Model loading parameters\n",
    "model_kwargs = {'device': 'cpu'}  # You can change to 'cuda' if you have a GPU\n",
    "encode_kwargs = {'normalize_embeddings': False}  # Do not normalize embeddings\n",
    "\n",
    "# Create the embedding object\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create a FAISS vector store and load the text chunks into it\n",
    "vector_store = FAISS.from_documents(split_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9VuTo1KeIN8o"
   },
   "outputs": [],
   "source": [
    "# Set up the retriever: for each query, retrieve the 5 most relevant chunks\n",
    "embedding_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C9MXoO6tSmXZ"
   },
   "outputs": [],
   "source": [
    "# Import function to create a chain that injects documents into the prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Import prompt template for the language model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import GigaChat â€” Sberâ€™s language model\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "# Import function to build a Retrieval-Augmented Generation (RAG) chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bYEwGa7VSwoo"
   },
   "outputs": [],
   "source": [
    "# Create a GigaChat object â€” specify model parameters and authentication\n",
    "llm = GigaChat(\n",
    "    credentials='LLM_API_KEY',\n",
    "#   model=\"GigaChat-Max\",\n",
    "    scope=\"GIGACHAT_API_PERS\",\n",
    "    verify_ssl_certs=False, \n",
    "    profanity_check=False\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "llm.invoke(\"Hello! Whatâ€™s your name?\").content\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "Answer the userâ€™s question as a friendly restaurant assistant. \n",
    "Use only the information provided in the context. \n",
    "If the context lacks an answer, politely inform the user.\n",
    "\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "Answer:\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j-0vNDWNcqHz"
   },
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QXTUO4XfcySD"
   },
   "outputs": [],
   "source": [
    "# Example query to our RAG system\n",
    "q1 = \"Is there any information about photons?\"\n",
    "\n",
    "# Pass the query to the chain\n",
    "resp1 = retrieval_chain.invoke(\n",
    "    {'input': q1}\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "resp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGqnoxm8GYdp"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W7B910Tg1MGy"
   },
   "outputs": [],
   "source": [
    "!pip install -q pyTelegramBotAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "79miQbh46i8K"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Simple prompt: the model answers only based on the provided context\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "You are a physics expert. Use only the context provided below.\n",
    "For greetings, reply: \"Ready to answer questions about physics news!\"\n",
    "If the context does not contain an answer, say that you cannot answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Answer:\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BKK8KqDe1XrX"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# LangChain chain that injects documents into the prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Main Retrieval-Augmented Generation (RAG) chain\n",
    "retrieval_chain = create_retrieval_chain(embedding_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AizNgpis2hzV"
   },
   "outputs": [],
   "source": [
    "import telebot\n",
    "from time import sleep\n",
    "\n",
    "bot = telebot.TeleBot('Bot_API_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qiOnAOkj3NlG"
   },
   "outputs": [],
   "source": [
    "# Handle non-text messages\n",
    "@bot.message_handler(content_types=['audio', 'video', 'document', 'photo',\n",
    "                                    'sticker', 'voice', 'location', 'contact'])\n",
    "def not_text(message):\n",
    "    bot.send_message(message.chat.id, \"I only work with text messages.\")\n",
    "\n",
    "# Handle text messages\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def handle_text_message(message):\n",
    "    user_id = message.chat.id\n",
    "    query = message.text\n",
    "\n",
    "    # Send the query to the retrieval_chain (RAG)\n",
    "    response = retrieval_chain.invoke({'input': query})\n",
    "\n",
    "    # Send the answer back to the user\n",
    "    bot.send_message(user_id, response['answer'])\n",
    "\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z-Zde2Eg0nbq"
   },
   "outputs": [],
   "source": [
    "bot.polling(none_stop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
