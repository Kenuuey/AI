{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-gigachat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_gigachat.chat_models import GigaChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "zNJbXDWcWXLH",
    "outputId": "43ff0a4a-7163-469a-9175-b19283b936b8"
   },
   "outputs": [],
   "source": [
    "llm = GigaChat(\n",
    "#             model=\"GigaChat-Max\",\n",
    "              credentials=\"API_key\",\n",
    "              scope=\"GIGACHAT_API_PERS\"\n",
    "              verify_ssl_certs=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfgEROmqo3RK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "Getting the Bot Token. We need a Telegram bot. Main steps:\n",
    "- Find the @BotFather account in Telegram\n",
    "- Send the command /newbot\n",
    "- Follow BotFather‚Äôs instructions to choose a name (any) and a username (unique, must include ‚Äúbot‚Äù in it)\n",
    "- After creating the bot, BotFather will give you a token to access the Telegram Bot API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa3lA48BOuJ9",
    "outputId": "578bde6b-6467-4787-d631-d31accf061f0"
   },
   "outputs": [],
   "source": [
    "!pip install -q pyTelegramBotAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXmtxn6urk_B"
   },
   "source": [
    "Working with Memory (Context Window) in LangChain  \r\n",
    "  \r\n",
    "\r\n",
    "Language models don‚Äôt have memory by default. They simply generate new text based on the prompt they receive. The prompt may contain the entire dialogue history starting from the system message or the first user message. But as the dialogue grows, each new user query is preceded by an increasingly long conversation history. This leads to:  \r\n",
    "- High token usage  \r\n",
    "- Risk of exceeding the model‚Äôs context window  \r\n",
    "\r\n",
    "To avoid this and optimize token usage, LangChain provides different **types of meory**:  \r\n",
    "\r\n",
    "- **`ConversationBufferMemory`** ‚Äì simplest option, stores and passes all dialogue messages.  \r\n",
    "- **`ConversationBufferWindowMemory`** ‚Äì similar to above, but keeps only the last *k* message pairs (user + model), acting as a sliding window.  \r\n",
    "- **`ConversationEntityMemory`** ‚Äì stores both dialogue history and extracted structured entities (names, objects, etc.) alongside unstructured text.  \r\n",
    "- **`ConversationKGMemory`** ‚Äì keeps memory as a **knowledge graph**.  \r\n",
    "- **`ConversationSummaryMemory`** ‚Äì keeps a **summary of the dialogue** instead of full messages. The model periodically updates the summary with new details.  \r\n",
    "- **`ConversationSummaryBufferMemory`** ‚Äì combines a limited message history with an overall summary of the conversation.  \r\n",
    "- **`ConversationTokenBufferMemory`** ‚Äì similar to window memory, but limits based on **number of tokens** instead of messages.  \r\n",
    "- **`VectorStoreRetrieverMemory`** ‚Äì stores dialogue in a vector database and retrieves only **semantically relevant** messages to include in context.  \r\n",
    "\r\n",
    "üëâ In practice:  \r\n",
    "- Use **Buffer/Window memory** for simple chatbots.  \r\n",
    "- Use **Summary memory** to scale long conversations.  \r\n",
    "- Use **Vector memory** when retrieval of specific past facts is needed.  \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz9mRiUtrw4d"
   },
   "source": [
    "ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P3wCD7YRrwgc"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcMk2z-xr7GD",
    "outputId": "68b0e9f0-4609-4931-b7c9-b5a154fc4ec3"
   },
   "outputs": [],
   "source": [
    "conversation1 = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "DQ-P396vuHOI",
    "outputId": "e643e829-ee2a-4ed1-be0f-e2e7d9196c60"
   },
   "outputs": [],
   "source": [
    "conversation1.predict(input=\"Hello! My name is John. I‚Äôm currently building a chatbot with LangChain and Python.\")\n",
    "conversation1.predict(input=\"Which large language model is considered the most advanced right now?\")\n",
    "conversation1.predict(input=\"I‚Äôd like to use a language model for free and write in English.\")\n",
    "conversation1.predict(input=\"What‚Äôs my name and what project am I working on?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJB8jKrawjZS"
   },
   "source": [
    "ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wI9DGShVw6Ap"
   },
   "outputs": [],
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfL-jM-uySAd",
    "outputId": "69d1c998-0996-4efd-a36e-c3c21739a53f"
   },
   "outputs": [],
   "source": [
    "conversation2 = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "NnVPIFboy7aB",
    "outputId": "a2958bfc-b24e-4806-c422-9cc5ea84a776"
   },
   "outputs": [],
   "source": [
    "conversation2.predict(input=\"Hello! My name is John. I‚Äôm currently building a chatbot with LangChain and Python.\")\n",
    "conversation2.predict(input=\"Which large language model is considered the most advanced right now?\")\n",
    "conversation2.predict(input=\"I‚Äôd like to use a language model for free and write in English.\")\n",
    "conversation2.predict(input=\"What‚Äôs my name and what project am I working on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyFleqRDzpzq",
    "outputId": "0a494c50-b946-43e3-de3a-b270bc675f55"
   },
   "outputs": [],
   "source": [
    "conversation2.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR4yw2WPzx8G"
   },
   "source": [
    "Create a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xyopldQJ3mVm"
   },
   "outputs": [],
   "source": [
    "user_conversations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC9SpPDM3sf9"
   },
   "outputs": [],
   "source": [
    "import telebot\n",
    "from time import sleep\n",
    "\n",
    "bot = telebot.TeleBot('Bot_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jyA7Ctkl3-v_"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uSNDXItw4xz5"
   },
   "outputs": [],
   "source": [
    "@bot.message_handler(content_types=['audio',\n",
    "                                    'video',\n",
    "                                    'document',\n",
    "                                    'photo',\n",
    "                                    'sticker',\n",
    "                                    'voice',\n",
    "                                    'location',\n",
    "                                    'contact'])\n",
    "def not_text(message):\n",
    "    user_id = message.chat.id\n",
    "    bot.send_message(user_id, \"I don‚Äôt work with non-text messages!\")\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def handle_text_message(message):\n",
    "    user_id = message.chat.id\n",
    "    if user_id not in user_conversations:\n",
    "        user_conversations[user_id] = ConversationBufferMemory()\n",
    "    conversation.memory = user_conversations[user_id]\n",
    "    response = conversation.predict(input=message.text)\n",
    "    bot.send_message(user_id, \"I won‚Äôt ignore your message, but I cannot avoid responding.\")\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ChYbe9xnJisi"
   },
   "outputs": [],
   "source": [
    "template = '''\n",
    "You are a chatbot answering questions as a consultant for SoftAI Solutions.  \n",
    "Your reply must be no longer than 20 words. Use only the data below:  \n",
    "SoftAI Solutions develops AI software and analytics tools.  \n",
    "Phone: +1 (555) 123-4567. Email: support@softai.com.  \n",
    "We are based in New York, 45 Innovation Street.  \n",
    "Products: corporate AI platforms, chatbots, and data analytics.  \n",
    "Purchases: via website softai.com or by phone.  \n",
    "Refunds: available within 14 days if the product is intact.  \n",
    "Complaints and feedback: send to feedback@softai.com.  \n",
    "For greetings reply: \"How can I help? I‚Äôm here to answer questions about SoftAI Solutions.\"  \n",
    "For any unrelated queries reply: \"Sorry, I only provide information about SoftAI Solutions.\"  \n",
    "\n",
    "\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:\n",
    "'''\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.prompt.template = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzaMM1JBJs4b",
    "outputId": "911f989f-4f60-4ad1-f5d6-253ce3d72860"
   },
   "outputs": [],
   "source": [
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rYENVst8VwOQ"
   },
   "outputs": [],
   "source": [
    "bot.polling(none_stop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
